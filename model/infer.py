# model/infer.py

import argparse
import json
import os
import sys

import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# ====== à¹à¸à¹‰ import à¹ƒà¸«à¹‰à¸¡à¸­à¸‡à¹€à¸«à¹‡à¸™ preprocessing.py à¹à¸™à¹ˆà¸™à¸­à¸™ ======
# à¹€à¸žà¸´à¹ˆà¸¡ path à¸‚à¸­à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ project (à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸—à¸µà¹ˆà¸¡à¸µ main.py, preprocessing.py)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from preprocessing import clean_text  # à¹ƒà¸Šà¹‰ clean_text à¸•à¸±à¸§à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š preprocessing.py

LABELS = ["profanity", "sexual", "violence", "hate"]


# ===== 1) à¸ªà¸£à¹‰à¸²à¸‡ context à¸£à¸­à¸š à¹† segment =====
def build_context(df: pd.DataFrame, window: int = 0) -> pd.DataFrame:
    """
    à¸–à¹‰à¸² window = 0 â†’ à¹ƒà¸Šà¹‰à¹€à¸‰à¸žà¸²à¸° text à¹€à¸”à¸µà¹ˆà¸¢à¸§ à¹†
    à¸–à¹‰à¸² window = 1 â†’ à¹ƒà¸Šà¹‰ prev [SEP] curr [SEP] next à¹€à¸«à¸¡à¸·à¸­à¸™à¹€à¸§à¸­à¸£à¹Œà¸Šà¸±à¸™à¹€à¸”à¸´à¸¡
    """
    texts = df["text"].astype(str).tolist()
    ctxs = []

    if window <= 0:
        ctxs = texts
    else:
        for i, t in enumerate(texts):
            prev_t = texts[i - 1] if i - 1 >= 0 else ""
            next_t = texts[i + 1] if i + 1 < len(texts) else ""
            ctx = prev_t + " [SEP] " + t + " [SEP] " + next_t
            ctxs.append(ctx)

    out = df.copy()
    out["text_ctx"] = ctxs
    return out


# ===== 2) à¹‚à¸«à¸¥à¸” threshold à¹à¸šà¸šà¸¢à¸·à¸”à¸«à¸¢à¸¸à¹ˆà¸™ =====
def load_thresholds(model_dir: str) -> np.ndarray:
    """
    thresholds_per_label.json à¸­à¸²à¸ˆà¹€à¸à¹‡à¸šà¹„à¸”à¹‰ 2 à¹à¸šà¸š:
    1) {"thresholds": [.. list à¸•à¸²à¸¡à¸¥à¸³à¸”à¸±à¸š LABELS ..]}
    2) {"profanity": 0.4, "sexual": 0.7, ...}  (dict à¸•à¸²à¸¡à¸Šà¸·à¹ˆà¸­ label)

    à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸™à¸µà¹‰à¸ˆà¸°à¸„à¸·à¸™ np.array(thresholds) à¸•à¸²à¸¡à¸¥à¸³à¸”à¸±à¸š LABELS à¹€à¸ªà¸¡à¸­
    """
    thr_path = os.path.join(model_dir, "thresholds_per_label.json")
    print(f"ðŸ“¥ Loading thresholds from: {thr_path}")

    with open(thr_path, "r", encoding="utf-8") as f:
        cfg = json.load(f)

    thr_dict = {}

    if isinstance(cfg, dict) and "thresholds" in cfg:
        # à¸à¸£à¸“à¸µà¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ list à¸•à¸£à¸‡ à¹†
        arr = np.array(cfg["thresholds"], dtype=float)
        if arr.shape[0] != len(LABELS):
            raise ValueError(
                f"à¸ˆà¸³à¸™à¸§à¸™ thresholds ({arr.shape[0]}) à¹„à¸¡à¹ˆà¸•à¸£à¸‡à¸à¸±à¸š LABELS ({len(LABELS)})"
            )
        thr_dict = dict(zip(LABELS, arr))
    elif isinstance(cfg, dict):
        # à¸à¸£à¸“à¸µà¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ dict à¸•à¸²à¸¡ label
        for lab in LABELS:
            if lab not in cfg:
                raise ValueError(
                    f"thresholds_per_label.json à¹„à¸¡à¹ˆà¸¡à¸µ key à¸ªà¸³à¸«à¸£à¸±à¸š label '{lab}'"
                )
            thr_dict[lab] = float(cfg[lab])
    else:
        raise ValueError("thresholds_per_label.json à¸¡à¸µà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹„à¸¡à¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡")

    thrs = np.array([thr_dict[lab] for lab in LABELS], dtype=float)
    print("ðŸ”§ Per-label thresholds:", thr_dict)
    return thrs


# ===== 3) main infer =====
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--model_dir",
        required=True,
        help="à¹€à¸Šà¹ˆà¸™ tox_ft\\best_model à¸«à¸£à¸·à¸­ tox_ft_pos\\best_model",
    )
    ap.add_argument(
        "--csv",
        required=True,
        help="à¹„à¸Ÿà¸¥à¹Œ transcript CSV (à¹€à¸Šà¹ˆà¸™ start_time,end_time,text)",
    )
    ap.add_argument(
        "--out",
        required=True,
        help="à¹„à¸Ÿà¸¥à¹Œà¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸£à¸§à¸¡à¸„à¸°à¹à¸™à¸™ (CSV)",
    )
    ap.add_argument(
        "--flag_out",
        required=True,
        help="à¹„à¸Ÿà¸¥à¹Œà¹€à¸‰à¸žà¸²à¸°à¹à¸–à¸§à¸—à¸µà¹ˆà¹‚à¸”à¸™ flag à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 1 label",
    )
    ap.add_argument(
        "--max_len",
        type=int,
        default=256,
        help="max sequence length à¸ªà¸³à¸«à¸£à¸±à¸š tokenizer",
    )
    ap.add_argument(
        "--context_window",
        type=int,
        default=0,
        help="0 = à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ context, 1 = à¹ƒà¸Šà¹‰ prev/next + [SEP]",
    )
    args = ap.parse_args()

    # ----- à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥ -----
    print(f"ðŸ“¦ Loading model from: {args.model_dir}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_dir)
    model = AutoModelForSequenceClassification.from_pretrained(args.model_dir)

    # ----- à¹‚à¸«à¸¥à¸” thresholds -----
    thrs = load_thresholds(args.model_dir)

    # ----- à¹‚à¸«à¸¥à¸” CSV -----
    print(f"ðŸ“„ Loading CSV: {args.csv}")
    df = pd.read_csv(args.csv).fillna("")
    lower_cols = {c.lower(): c for c in df.columns}

    if "text" in lower_cols:
        text_col = lower_cols["text"]
    elif "transcript" in lower_cols:
        text_col = lower_cols["transcript"]
    else:
        raise ValueError("CSV à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ 'text' à¸«à¸£à¸·à¸­ 'transcript'")

    # ----- à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸” text à¸”à¹‰à¸§à¸¢ clean_text à¸ˆà¸²à¸ preprocessing.py -----
    print("ðŸ§¹ Cleaning text...")
    df = df.copy()
    df["text"] = df[text_col].astype(str).apply(clean_text)

    # ----- à¸ªà¸£à¹‰à¸²à¸‡ context -----
    print(f"ðŸ§± Building context (window={args.context_window})...")
    df = build_context(df, window=args.context_window)
    texts = df["text_ctx"].astype(str).tolist()

    # ----- Tokenization -----
    print("ðŸ¤– Running inference...")
    enc = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=args.max_len,
        return_tensors="pt",
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    enc = {k: v.to(device) for k, v in enc.items()}

    model.eval()
    with torch.no_grad():
        logits = model(**enc).logits.detach().cpu().numpy()

    # ----- Sigmoid â†’ prob -----
    probs = 1.0 / (1.0 + np.exp(-logits))

    # broadcast thresholds à¹ƒà¸«à¹‰à¹€à¸—à¹ˆà¸²à¸£à¸¹à¸› logits (N, num_labels)
    preds = (probs >= thrs[None, :]).astype(int)

    # ----- à¹€à¸‚à¸µà¸¢à¸™à¸œà¸¥à¸à¸¥à¸±à¸šà¸¥à¸‡ df -----
    for i, lab in enumerate(LABELS):
        df[lab] = preds[:, i]
        df[f"{lab}_prob"] = probs[:, i]

    # ----- à¹€à¸‹à¸Ÿà¹„à¸Ÿà¸¥à¹Œ -----
    out_dir = os.path.dirname(args.out)
    flag_dir = os.path.dirname(args.flag_out)
    if out_dir:
        os.makedirs(out_dir, exist_ok=True)
    if flag_dir:
        os.makedirs(flag_dir, exist_ok=True)

    df.to_csv(args.out, index=False)
    flagged = df[df[LABELS].sum(axis=1) > 0]
    flagged.to_csv(args.flag_out, index=False)

    print("âœ… Saved full results to:", args.out)
    print("âœ… Saved flagged rows to:", args.flag_out)
    print("ðŸ”¢ Flagged segments:", len(flagged))


if __name__ == "__main__":
    main()
