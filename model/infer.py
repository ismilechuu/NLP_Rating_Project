# model/infer_v2.py

import argparse
import json
import re

import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

LABELS = ["profanity", "sexual", "violence", "hate"]

# ===== 1) Advanced clean_text (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏∏‡πà‡∏°‡∏ô‡∏ß‡∏•‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°) =====

LEET_MAP = str.maketrans({
    "@": "a",
    "$": "s",
    "0": "o",
    "1": "i",
    "!": "i",
    "3": "e",
    "4": "a",
    "7": "t",
})

PROFANITY_PATTERNS = [
    (r"f[\W_]*u[\W_]*c[\W_]*k+", "fuck"),
    (r"f+[\W_]*\*+[\W_]*k+", "fuck"),
    (r"s[\W_]*h[\W_]*i[\W_]*t+", "shit"),
    (r"b[\W_]*i[\W_]*t[\W_]*c[\W_]*h+", "bitch"),
    (r"a[\W_]*s[\W_]*s[\W_]*h[\W_]*o[\W_]*l[\W_]*e+", "asshole"),
    (r"a[\W_]*s[\W_]*s+", "ass"),
    (r"d[\W_]*a[\W_]*m[\W_]*n+", "damn"),
    (r"p[\W_]*o[\W_]*r[\W_]*n+", "porn"),
]

URL_RE = re.compile(r"http\S+|www\.\S+")

def clean_text_advanced(s: str) -> str:
    s = str(s).strip()
    # normalize quotes
    s = s.replace("‚Äô", "'").replace("‚Äú", '"').replace("‚Äù", '"')
    s = s.lower()
    # remove URL
    s = URL_RE.sub(" ", s)
    # leet
    s = s.translate(LEET_MAP)
    # remove emoji / high unicode (‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏¢‡∏∏‡πà‡∏á‡∏Å‡∏±‡∏ö ! . , ?)
    s = re.sub(r"[\U00010000-\U0010ffff]", " ", s)
    # ‡∏•‡∏ö censor ‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏∏‡πà‡∏°‡∏ô‡∏ß‡∏• ‡πÄ‡∏ä‡πà‡∏ô f***, b.i.t.c.h ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡∏°‡∏±‡πà‡∏ß
    s = re.sub(r"(?<=\w)[\*_]+(?=\w)", "", s)   # ‡∏•‡∏ö * / _ ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
    s = re.sub(r"(?<=\w)\.(?=\w)", "", s)       # ‡∏•‡∏ö . ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (b.i.t.c.h -> bitch)

    # apply profanity patterns
    for pat, repl in PROFANITY_PATTERNS:
        s = re.sub(pat, repl, s)

    # normalize space ‡∏£‡∏≠‡∏ö punctuation ‡∏´‡∏•‡∏±‡∏Å
    s = re.sub(r"\s+([,\.!?])", r"\1", s)      # space ‡∏Å‡πà‡∏≠‡∏ô , . ! ?
    s = re.sub(r"([,\.!?])([^\s])", r"\1 \2", s)  # space ‡∏´‡∏•‡∏±‡∏á , . ! ?
    # collapse spaces
    s = re.sub(r"\s+", " ", s).strip()
    return s

# ===== 2) context window (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å args) =====

def build_context(df: pd.DataFrame, window: int = 0) -> pd.DataFrame:
    """
    ‡∏ñ‡πâ‡∏≤ window=0 ‚Üí ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ text ‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß ‡πÜ
    ‡∏ñ‡πâ‡∏≤ window=1 ‚Üí ‡πÉ‡∏ä‡πâ prev [SEP] curr [SEP] next (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
    """
    texts = df["text"].astype(str).tolist()
    ctxs = []
    if window <= 0:
        # no context
        ctxs = texts
    else:
        for i, t in enumerate(texts):
            prev_t = texts[i - 1] if i - 1 >= 0 else ""
            next_t = texts[i + 1] if i + 1 < len(texts) else ""
            ctx = prev_t + " [SEP] " + t + " [SEP] " + next_t
            ctxs.append(ctx)
    out = df.copy()
    out["text_ctx"] = ctxs
    return out

# ===== 3) main =====

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model_dir", required=True,
                    help="‡πÄ‡∏ä‡πà‡∏ô tox_ft\\best_model ‡∏´‡∏£‡∏∑‡∏≠ tox_ft_pos\\best_model")
    ap.add_argument("--csv", required=True,
                    help="‡πÑ‡∏ü‡∏•‡πå transcript CSV (start_time,end_time,text)")
    ap.add_argument("--out", required=True,
                    help="‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô")
    ap.add_argument("--flag_out", required=True,
                    help="‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÇ‡∏î‡∏ô flag")
    ap.add_argument("--max_len", type=int, default=256)
    ap.add_argument("--context_window", type=int, default=0,
                    help="0 = ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ context, 1 = ‡πÉ‡∏ä‡πâ prev/next + [SEP]")
    args = ap.parse_args()

    print(f"üì¶ Loading model from: {args.model_dir}")
    tok = AutoTokenizer.from_pretrained(args.model_dir)
    model = AutoModelForSequenceClassification.from_pretrained(args.model_dir)

    thr_path = f"{args.model_dir}/thresholds_per_label.json"
    print(f"üì• Loading thresholds from: {thr_path}")
    with open(thr_path, "r") as f:
        cfg = json.load(f)
    thrs = np.array(cfg["thresholds"], dtype=float)
    print("üîß Per-label thresholds:", dict(zip(LABELS, thrs)))

    print(f"üìÑ Loading CSV: {args.csv}")
    df = pd.read_csv(args.csv).fillna("")
    lower_cols = {c.lower(): c for c in df.columns}

    if "text" in lower_cols:
        text_col = lower_cols["text"]
    elif "transcript" in lower_cols:
        text_col = lower_cols["transcript"]
    else:
        raise ValueError("CSV ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'text' ‡∏´‡∏£‡∏∑‡∏≠ 'transcript'")

    print("üßπ Cleaning text...")
    df = df.copy()
    df["text"] = df[text_col].astype(str).apply(clean_text_advanced)

    # context
    print(f"üß± Building context (window={args.context_window})...")
    df = build_context(df, window=args.context_window)
    texts = df["text_ctx"].astype(str).tolist()

    # tokenization
    print("ü§ñ Running inference...")
    enc = tok(
        texts,
        truncation=True,
        padding=True,
        max_length=args.max_len,
        return_tensors="pt",
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    enc = {k: v.to(device) for k, v in enc.items()}
    model.eval()
    with torch.no_grad():
        logits = model(**enc).logits.detach().cpu().numpy()

    probs = 1 / (1 + np.exp(-logits))
    preds = (probs >= thrs).astype(int)

    for i, lab in enumerate(LABELS):
        df[lab] = preds[:, i]
        df[f"{lab}_prob"] = probs[:, i]

    df.to_csv(args.out, index=False)
    flagged = df[df[LABELS].sum(axis=1) > 0]
    flagged.to_csv(args.flag_out, index=False)

    print("‚úÖ Saved full results to:", args.out)
    print("‚úÖ Saved flagged rows to:", args.flag_out)
    print("üî¢ Flagged segments:", len(flagged))


if __name__ == "__main__":
    main()

# python model\infer.py --model_dir tox_ft\best_model --csv "Transcript\STOP BEING A PEOPLE PLEASER - Gary Vaynerchuk Motivation.csv" --out outputs/transcript_with_scores.csv --flag_out outputs/transcript_flagged.csv --context_window 0